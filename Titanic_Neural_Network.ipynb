{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c286740c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/francisco/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b363fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    titles = []\n",
    "    for name in df['Name']:\n",
    "        title = name.split(',')[-1].split('.')[0]\n",
    "        titles.append(title[1:])\n",
    "    df['Name'] = titles\n",
    "    df.rename(columns = {'Name':'Title'}, inplace = True)\n",
    "\n",
    "    decks = []\n",
    "    for cabin in df['Cabin']:\n",
    "        if pd.notna(cabin): decks.append(cabin[0])\n",
    "        else: decks.append(cabin)\n",
    "    df['Cabin'] = decks\n",
    "    df.rename(columns = {'Cabin':'Deck'}, inplace = True)\n",
    "\n",
    "#     fam_size  = []\n",
    "#     for i in range(len(df)):\n",
    "#         fam_size.append(df['SibSp'][i]+df['Parch'][i])\n",
    "#     df['FamSize'] = fam_size\n",
    "\n",
    "    fare_per_person = []\n",
    "    for i in range(len(df)):\n",
    "        fam_size = df['SibSp'][i]+df['Parch'][i]\n",
    "        fare_per_person.append(df['Fare'][i]/(fam_size+1))\n",
    "    df['FarePerPerson'] = fare_per_person\n",
    "    \n",
    "    \n",
    "#     age_times_class = []\n",
    "#     for i in range(len(df)):\n",
    "#         age_times_class.append(df['Pclass'][i]*df['Age'][i])\n",
    "#     df['AgeClass'] = age_times_class\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456a7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorify(df, columns):\n",
    "    rectifier = 1\n",
    "    for column in columns:\n",
    "        category_dict = {}\n",
    "        unique_categories = df[column].unique()\n",
    "        unknown_index = (unique_categories != unique_categories).nonzero()[0]\n",
    "        unique_categories = unique_categories.tolist()\n",
    "\n",
    "        if len(unknown_index): \n",
    "            category_indexes = list(range(len(unique_categories)))\n",
    "            unknown = unique_categories[unknown_index[0]]\n",
    "            category_indexes[0] = unknown\n",
    "            unique_categories.pop(unknown_index[0])\n",
    "            unique_categories.insert(0,unknown)\n",
    "\n",
    "        else:\n",
    "            category_indexes = list(range(1,len(unique_categories)+1))\n",
    "\n",
    "        for i in range(len(unique_categories)):\n",
    "            category_dict[unique_categories[i]] = category_indexes[i]\n",
    "\n",
    "        new_column = []\n",
    "        for i in range(len(df)):\n",
    "            new_value = category_dict[df[column][i]]\n",
    "            if new_value == new_value: new_value = int(new_value)\n",
    "            new_column.append(new_value)\n",
    "\n",
    "        df[column] = new_column\n",
    "#         df[column] = new_column.astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369d9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df, ignore_columns):\n",
    "    for column in df.columns:\n",
    "        if column in ignore_columns: continue\n",
    "        if df[column].isna().any():\n",
    "            median = df[column].describe()['50%']\n",
    "            nan_indexes = df[column].isna().values\n",
    "            new_column = np.array(df[column])\n",
    "            new_column[nan_indexes] = median\n",
    "#             new_column = new_column.astype('int')\n",
    "            df[column] = new_column\n",
    "            df[f'{column}Missing'] = nan_indexes \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82be6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integerize(df, columns):\n",
    "    for column in columns:\n",
    "        df = df.astype({column: 'int'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9716ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = complete_df[column].values / complete_df[column].values.max()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a51f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTabular(Dataset):\n",
    "    def __init__(self, df, cont, cat, target_key = None):\n",
    "        self.df = df\n",
    "        self.cat = cat\n",
    "        self.cont = cont\n",
    "#         self.cardinalities = [len(df[column].unique()) for column in cat]\n",
    "        self.cardinalities = [complete_df[column].nunique() for column in cat]\n",
    "#         self.target_cardinality = None\n",
    "        self.target_key = target_key\n",
    "#         if target is not None: self.target_cardinality = len(df[target].unique()) - 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        vectors_to_concat = []\n",
    "        for j, column in enumerate(self.cat):\n",
    "            one_hot_vector = torch.zeros(self.cardinalities[j], dtype=torch.int)\n",
    "            one_hot_vector[self.df[column][i] - 1] = 1 \n",
    "            vectors_to_concat.append(one_hot_vector)\n",
    "            \n",
    "        for column in self.cont:\n",
    "            vectors_to_concat.append(torch.tensor([self.df[column][i]]))\n",
    "        \n",
    "        input_vector = torch.cat(vectors_to_concat)\n",
    "        \n",
    "        if self.target_key is not None:\n",
    "            target = torch.tensor([self.df[self.target_key][i]])\n",
    "            return input_vector.float(), target\n",
    "        \n",
    "#         if self.target_cardinality is not None:\n",
    "#             out_vector = torch.zeros(self.target_cardinality)\n",
    "#             out_vector[self.df[self.target][i] - 1] = 1\n",
    "#             return input_vector, out_vector\n",
    "        \n",
    "        return input_vector.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095cc546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.n_layers = len(sizes)-1\n",
    "        self.linear = nn.ModuleList([nn.Linear(sizes[i], sizes[i+1]) for i in range(self.n_layers)])\n",
    "        self.batchnorms = nn.ModuleList([nn.BatchNorm1d(sizes[i+1]) for i in range(self.n_layers)])\n",
    "    \n",
    "    def linear_block(self, x, idx):\n",
    "        x = self.linear[idx](x)\n",
    "        x = self.batchnorms[idx](x)\n",
    "        if idx != self.n_layers-1:\n",
    "            x = torch.sigmoid(x) \n",
    "        return  x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.linear_block(x,i)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce1578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(predictions, targets):\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5d577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(predictions, targets):\n",
    "    return 100*(((predictions > 0) == targets).count_nonzero() / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41109b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndEvaluate():\n",
    "    def __init__(self, model, optimizer, loss_func, metric, train_dataloader, valid_dataloader):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_func = loss_func\n",
    "        self.train_dl = train_dataloader\n",
    "        self.valid_dl = valid_dataloader\n",
    "        self.metric = metric\n",
    "    \n",
    "    def train_and_evaluate(self, n_epoch):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            kbar = pkbar.Kbar(target=(len(self.train_dl)+len(self.valid_dl)), epoch=epoch, num_epochs=n_epoch, width=16, stateful_metrics=['Validation Loss','Accuracy'])\n",
    "            running_loss_t = 0.0\n",
    "            running_loss_v = 0.0\n",
    "            \n",
    "            for i, b in enumerate(self.train_dl):\n",
    "                input_vector = b[0]\n",
    "                target = b[1]\n",
    "                preds = self.model(input_vector)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_func(preds, target)\n",
    "                \n",
    "                loss.backward() \n",
    "                self.optimizer.step()\n",
    "                running_loss_t = loss.item()\n",
    "                kbar.update(i, values=[(\"Train Loss\", running_loss_t)])\n",
    "            \n",
    "            kbar.add(1, values=[(\"Validation Loss\", 0), (\"Accuracy\", 0)])        \n",
    "            with torch.no_grad():\n",
    "                for j, b in enumerate(self.valid_dl):\n",
    "                    input_vector = b[0]\n",
    "                    target = b[1]\n",
    "\n",
    "                    preds = self.model(input_vector)\n",
    "                    loss = self.loss_func(preds, target)\n",
    "                    running_loss_v = loss\n",
    "                    accuracy = self.metric(preds, target)\n",
    "                    kbar.update(i+j, values=[(\"Validation Loss\", running_loss_v), (\"Accuracy\", accuracy)])\n",
    "        print('Finished Training')\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6def5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/francisco/workspace/titanic_kaggle/titanic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db16aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{path}/train.csv',low_memory=False)\n",
    "test_df = pd.read_csv(f'{path}/test.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8c44f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c157f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_df = feature_engineer(complete_df)\n",
    "# complete_df = complete_df.drop(['Ticket', 'Fare'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa311649",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = ['Age', 'FarePerPerson', 'SibSp' , 'Parch']\n",
    "cat = ['Survived', 'Pclass', 'Title', 'Sex', 'Deck', 'Embarked', 'AgeMissing','FarePerPersonMissing','DeckMissing', 'EmbarkedMissing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e9c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = categorify(complete_df,['Title', 'Sex', 'Deck', 'Embarked'])\n",
    "complete_df = fill_missing(complete_df, ['Survived'])\n",
    "complete_df = normalize(complete_df,cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47db751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = complete_df.iloc[:712]\n",
    "valid_df = complete_df.iloc[712:891].reset_index()\n",
    "test_df = complete_df.iloc[891:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fa88a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = integerize(train_df, cat)\n",
    "# valid_df = integerize(valid_df, cat)\n",
    "# test_df = integerize(test_df, cat[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c509db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = DatasetTabular(train_df, cont, cat[1:], 'Survived')\n",
    "valid_dset = DatasetTabular(valid_df, cont, cat[1:], 'Survived')\n",
    "test_dset = DatasetTabular(test_df, cont, cat[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8cd63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dset, batch_size=128, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47649083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = LinearNet([46,128,128,128,128,64,64,32,32,16,16,8,8,2,2,1])\n",
    "# net = LinearNet([46,46,32,32,32,16,16,16,8,8,8,4,4,4,2,2,1])\n",
    "net = LinearNet([46,128,128,128,128,128,128,64,64,64,64,64,64,32,32,32,32,32,32,16,16,16,16,16,16,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "683778a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(net.parameters(), lr=0.00001)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.00001, weight_decay=10e-4)\n",
    "trainer = TrainAndEvaluate(net,optimizer,loss_func,metric,train_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76be85f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3154 - Validation Loss: 0.2980 - Accuracy: 92.1788Epoch: 2/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3131 - Validation Loss: 0.2980 - Accuracy: 92.1788Epoch: 3/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3135 - Validation Loss: 0.2981 - Accuracy: 92.1788Epoch: 4/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3120 - Validation Loss: 0.2982 - Accuracy: 91.6201Epoch: 5/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3067 - Validation Loss: 0.2980 - Accuracy: 92.7374Epoch: 6/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3088 - Validation Loss: 0.2979 - Accuracy: 92.1788Epoch: 7/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3097 - Validation Loss: 0.2981 - Accuracy: 92.7374Epoch: 8/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3015 - Validation Loss: 0.2983 - Accuracy: 92.7374Epoch: 9/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3088 - Validation Loss: 0.2984 - Accuracy: 92.1788Epoch: 10/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3038 - Validation Loss: 0.2981 - Accuracy: 92.1788Epoch: 11/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3120 - Validation Loss: 0.2983 - Accuracy: 92.1788Epoch: 12/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3061 - Validation Loss: 0.2984 - Accuracy: 91.6201Epoch: 13/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3148 - Validation Loss: 0.2984 - Accuracy: 91.6201Epoch: 14/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3063 - Validation Loss: 0.2983 - Accuracy: 92.7374Epoch: 15/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3057 - Validation Loss: 0.2982 - Accuracy: 92.7374Epoch: 16/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3043 - Validation Loss: 0.2982 - Accuracy: 92.1788Epoch: 17/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3076 - Validation Loss: 0.2983 - Accuracy: 92.1788Epoch: 18/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3063 - Validation Loss: 0.2981 - Accuracy: 92.1788Epoch: 19/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3121 - Validation Loss: 0.2983 - Accuracy: 92.1788Epoch: 20/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3095 - Validation Loss: 0.2988 - Accuracy: 91.6201Epoch: 21/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3094 - Validation Loss: 0.2985 - Accuracy: 91.6201Epoch: 22/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3026 - Validation Loss: 0.2983 - Accuracy: 91.6201Epoch: 23/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3066 - Validation Loss: 0.2979 - Accuracy: 92.1788Epoch: 24/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3103 - Validation Loss: 0.2978 - Accuracy: 92.1788Epoch: 25/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3107 - Validation Loss: 0.2977 - Accuracy: 92.7374Epoch: 26/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3082 - Validation Loss: 0.2974 - Accuracy: 92.7374Epoch: 27/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3084 - Validation Loss: 0.2975 - Accuracy: 92.7374Epoch: 28/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3063 - Validation Loss: 0.2974 - Accuracy: 92.1788Epoch: 29/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3075 - Validation Loss: 0.2974 - Accuracy: 92.7374Epoch: 30/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3116 - Validation Loss: 0.2976 - Accuracy: 92.1788Epoch: 31/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3091 - Validation Loss: 0.2979 - Accuracy: 92.1788Epoch: 32/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3037 - Validation Loss: 0.2982 - Accuracy: 91.0615Epoch: 33/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3098 - Validation Loss: 0.2987 - Accuracy: 91.6201Epoch: 34/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3127 - Validation Loss: 0.2990 - Accuracy: 91.6201Epoch: 35/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3149 - Validation Loss: 0.2993 - Accuracy: 91.6201Epoch: 36/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3098 - Validation Loss: 0.2989 - Accuracy: 91.6201Epoch: 37/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3071 - Validation Loss: 0.2984 - Accuracy: 92.1788Epoch: 38/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3077 - Validation Loss: 0.2982 - Accuracy: 92.1788Epoch: 39/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3097 - Validation Loss: 0.2978 - Accuracy: 92.1788Epoch: 40/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3052 - Validation Loss: 0.2978 - Accuracy: 92.1788Epoch: 41/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3047 - Validation Loss: 0.2978 - Accuracy: 92.7374Epoch: 42/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3060 - Validation Loss: 0.2976 - Accuracy: 92.7374Epoch: 43/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3025 - Validation Loss: 0.2973 - Accuracy: 92.1788Epoch: 44/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3069 - Validation Loss: 0.2973 - Accuracy: 91.6201Epoch: 45/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3057 - Validation Loss: 0.2977 - Accuracy: 91.6201Epoch: 46/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3074 - Validation Loss: 0.2979 - Accuracy: 92.1788Epoch: 47/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3089 - Validation Loss: 0.2978 - Accuracy: 92.1788Epoch: 48/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3051 - Validation Loss: 0.2974 - Accuracy: 92.1788Epoch: 49/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3066 - Validation Loss: 0.2969 - Accuracy: 92.1788Epoch: 50/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3049 - Validation Loss: 0.2969 - Accuracy: 92.1788Epoch: 51/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3072 - Validation Loss: 0.2973 - Accuracy: 92.1788Epoch: 52/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3065 - Validation Loss: 0.2978 - Accuracy: 92.1788Epoch: 53/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3072 - Validation Loss: 0.2975 - Accuracy: 92.7374Epoch: 54/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3118 - Validation Loss: 0.2971 - Accuracy: 92.1788Epoch: 55/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3100 - Validation Loss: 0.2971 - Accuracy: 92.1788Epoch: 56/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3049 - Validation Loss: 0.2968 - Accuracy: 92.1788Epoch: 57/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3110 - Validation Loss: 0.2968 - Accuracy: 92.1788Epoch: 58/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3110 - Validation Loss: 0.2965 - Accuracy: 92.7374Epoch: 59/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3083 - Validation Loss: 0.2965 - Accuracy: 92.7374Epoch: 60/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3081 - Validation Loss: 0.2966 - Accuracy: 92.7374Epoch: 61/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3024 - Validation Loss: 0.2966 - Accuracy: 92.7374Epoch: 62/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3107 - Validation Loss: 0.2970 - Accuracy: 92.7374Epoch: 63/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3007 - Validation Loss: 0.2973 - Accuracy: 92.1788Epoch: 64/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3019 - Validation Loss: 0.2975 - Accuracy: 91.6201Epoch: 65/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3073 - Validation Loss: 0.2979 - Accuracy: 91.6201Epoch: 66/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3059 - Validation Loss: 0.2977 - Accuracy: 92.1788Epoch: 67/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3142 - Validation Loss: 0.2974 - Accuracy: 92.1788Epoch: 68/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3097 - Validation Loss: 0.2974 - Accuracy: 92.7374Epoch: 69/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3056 - Validation Loss: 0.2972 - Accuracy: 92.7374Epoch: 70/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3032 - Validation Loss: 0.2966 - Accuracy: 93.8548Epoch: 71/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3109 - Validation Loss: 0.2962 - Accuracy: 93.2961Epoch: 72/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3102 - Validation Loss: 0.2963 - Accuracy: 93.8548Epoch: 73/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3061 - Validation Loss: 0.2968 - Accuracy: 93.8548Epoch: 74/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3092 - Validation Loss: 0.2972 - Accuracy: 92.1788Epoch: 75/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3054 - Validation Loss: 0.2976 - Accuracy: 92.1788Epoch: 76/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3067 - Validation Loss: 0.2977 - Accuracy: 92.1788Epoch: 77/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3091 - Validation Loss: 0.2975 - Accuracy: 92.7374Epoch: 78/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3023 - Validation Loss: 0.2972 - Accuracy: 92.7374Epoch: 79/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3089 - Validation Loss: 0.2970 - Accuracy: 92.7374Epoch: 80/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3053 - Validation Loss: 0.2968 - Accuracy: 93.8548Epoch: 81/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3035 - Validation Loss: 0.2966 - Accuracy: 93.8548Epoch: 82/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3099 - Validation Loss: 0.2965 - Accuracy: 93.8548Epoch: 83/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3057 - Validation Loss: 0.2962 - Accuracy: 93.2961Epoch: 84/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3039 - Validation Loss: 0.2961 - Accuracy: 92.7374Epoch: 85/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3078 - Validation Loss: 0.2961 - Accuracy: 92.7374Epoch: 86/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3062 - Validation Loss: 0.2961 - Accuracy: 92.7374Epoch: 87/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3022 - Validation Loss: 0.2963 - Accuracy: 93.2961Epoch: 88/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3039 - Validation Loss: 0.2962 - Accuracy: 92.7374Epoch: 89/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3080 - Validation Loss: 0.2962 - Accuracy: 92.7374Epoch: 90/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3050 - Validation Loss: 0.2963 - Accuracy: 92.7374Epoch: 91/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3035 - Validation Loss: 0.2967 - Accuracy: 93.2961Epoch: 92/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3113 - Validation Loss: 0.2966 - Accuracy: 92.7374Epoch: 93/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3044 - Validation Loss: 0.2961 - Accuracy: 92.1788Epoch: 94/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3097 - Validation Loss: 0.2961 - Accuracy: 92.7374Epoch: 95/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3072 - Validation Loss: 0.2961 - Accuracy: 92.7374Epoch: 96/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.2996 - Validation Loss: 0.2963 - Accuracy: 92.7374Epoch: 97/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3078 - Validation Loss: 0.2963 - Accuracy: 93.2961Epoch: 98/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3090 - Validation Loss: 0.2964 - Accuracy: 93.2961Epoch: 99/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3078 - Validation Loss: 0.2965 - Accuracy: 93.2961Epoch: 100/100\n",
      "5/7 [==========>.....] - ETA: 0s - Train Loss: 0.3067 - Validation Loss: 0.2962 - Accuracy: 92.7374Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = trainer.train_and_evaluate(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3ff47",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8055eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_dset,batch_size=418)\n",
    "\n",
    "preds_test = net(next(iter(test_dl)))\n",
    "\n",
    "preds_test[preds_test < 0] = 0\n",
    "preds_test[preds_test > 0] = 1\n",
    "\n",
    "preds_nn_df = pd.DataFrame()\n",
    "preds_nn_df['PassengerId'] = test_df['PassengerId'].values\n",
    "preds_nn_df['Survived'] = preds_test.int()\n",
    "\n",
    "preds_nn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0887725",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_nn_df.to_csv(f'{path}/submission_nn.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33dd47",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5bafae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = rf.predict(train_df.values[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "242b53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = train_df['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e49a588d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876543209876543"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(preds_train == gt)/len(gt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
